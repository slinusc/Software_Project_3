{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import datetime as dt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T20:27:33.115213800Z",
     "start_time": "2023-12-05T20:27:31.515395100Z"
    }
   },
   "id": "2ac714e4e5323226"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting up the MongoDB."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "844f5058f4a1d0e9"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "client = MongoClient(host=\"localhost\",port=27017)\n",
    "db = client[\"data_base_OSM\"]\n",
    "collection = db[\"bicycle_amenities\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T20:27:34.757472300Z",
     "start_time": "2023-12-05T20:27:34.689801100Z"
    }
   },
   "id": "bf90498cc361d74a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extracting the data of needed amenities from the OSM JSON file and loading it into the MongoDB."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f27d53aafe0b5ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_amenity(name_input_file, amenity_list, name_output_file):\n",
    "    if name_output_file + '.json' in os.listdir():\n",
    "        print(\"Filtered data already exists!\")\n",
    "    else:\n",
    "        with open(name_input_file + \".json\", 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        individual_nodes = []\n",
    "\n",
    "        for node in data['nodes']:\n",
    "            amenity = node.get('amenity')\n",
    "            if amenity in amenity_list:\n",
    "                individual_nodes.append({'node': node})\n",
    "\n",
    "        with open(name_output_file + '.json', 'w') as output_file:\n",
    "            json.dump(individual_nodes, output_file, indent=2)\n",
    "\n",
    "        print(\"Filtered data has been written to 'amenity_filtered.json'\")\n",
    "\n",
    "\n",
    "def load_mongo_db(file_name):\n",
    "    try:\n",
    "        collection.drop()\n",
    "        print(\"Collection has been droped!\")\n",
    "\n",
    "        # JSON-Daten aus einer Datei lesen\n",
    "        with open(file_name + '.json', 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        if isinstance(data, list):\n",
    "            collection.insert_many(data)\n",
    "        else:\n",
    "            collection.insert_one(data)\n",
    "        print(\"Data has been loaded to MongoDB!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        \n",
    "    \n",
    "amenities = [\"bicycle_parking\", \"bicycle_rental\", \"bicycle_repair_station\",\n",
    "                 \"compressed_air\", \"drinking_water\", \"shelter\"]\n",
    "\n",
    "extract_amenity('osm-output', amenities, 'amenity_filtered')\n",
    "\n",
    "load_mongo_db('../data/amenities_2023-11-17')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9617662b4a1e3c5e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adding cantons to the documents by using the OSM API."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa5a53f2ec5f2b3e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_canton(lat, lon):\n",
    "    try:\n",
    "        response = requests.get(f'https://nominatim.openstreetmap.org/reverse?format=json&lat={lat}&lon={lon}&zoom=18&addressdetails=1')\n",
    "        data = response.json()\n",
    "        return data['address']['state']\n",
    "    except:\n",
    "        print(\"Error: Could not get canton from OSM API\")\n",
    "        return \"No canton found\"\n",
    "\n",
    "\n",
    "def add_kanton_to_db():\n",
    "    counter = 0\n",
    "    \n",
    "    for doc in collection.find():\n",
    "\n",
    "        lat = float(doc['node']['lat'])  # String to float\n",
    "        lon = float(doc['node']['lon'])\n",
    "        canton = get_canton(lat, lon)\n",
    "        collection.update_one({'_id': doc['_id']}, {'$set': {'node.canton': canton}})\n",
    "        print(f\"{counter}, added canton '{canton}' to document with id '{doc['_id']}'\")\n",
    "        counter += 1\n",
    "    \n",
    "    print(\"Cantons have been added to the database!\")\n",
    "    \n",
    "add_kanton_to_db()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62eb01ad82742da0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are some POIs that are not in Switzerland. These will be deleted."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7f4372771364282"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted documents with canton '['Baden-Württemberg', 'Vorarlberg', 'Trentino-Alto Adige/Südtirol', 'Lombardia', 'Auvergne-Rhône-Alpes', 'No canton found', 'Grand Est', 'Bourgogne-Franche-Comté', \"Valle d'Aosta / Vallée d'Aoste\"]'\n"
     ]
    }
   ],
   "source": [
    "cantons = ['Aargau',\n",
    "              'Appenzell Ausserrhoden',\n",
    "              'Appenzell Innerrhoden',\n",
    "              'Basel-Landschaft',\n",
    "              'Basel-Stadt',\n",
    "              'Bern',\n",
    "              'Freiburg',\n",
    "              'Genf',\n",
    "              'Glarus',\n",
    "              'Graubünden',\n",
    "              'Jura',\n",
    "              'Luzern',\n",
    "              'Neuenburg',\n",
    "              'Nidwalden',\n",
    "              'Obwalden',\n",
    "              'Schaffhausen',\n",
    "              'Schwyz',\n",
    "              'Solothurn',\n",
    "              'St. Gallen',\n",
    "              'Tessin',\n",
    "              'Thurgau',\n",
    "              'Uri',\n",
    "              'Waadt',\n",
    "              'Wallis',\n",
    "              'Zug',\n",
    "              'Zürich']\n",
    "\n",
    "\n",
    "def delete_wrong_POIs(cantons, wrong_POIs):\n",
    "    wrong_POIs = collection.distinct(\"node.canton\", {\"node.canton\": {\"$nin\": cantons}})\n",
    "    collection.delete_many({\"node.canton\": {\"$nin\": cantons}})\n",
    "    print(f\"Deleted documents with wrong location: '{wrong_POIs}'\")\n",
    "\n",
    "\n",
    "delete_wrong_POIs(cantons)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-17T16:55:15.062810800Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The cantons with french or italian names will be translated to german."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "574234028a628d35"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated documents with canton 'Vaud' to 'Waadt'\n",
      "Updated documents with canton 'Graubünden/Grischun/Grigioni' to 'Graubünden'\n",
      "Updated documents with canton 'Bern/Berne' to 'Bern'\n",
      "Updated documents with canton 'Valais/Wallis' to 'Wallis'\n",
      "Updated documents with canton 'Neuchâtel' to 'Neuenburg'\n",
      "Updated documents with canton 'Ticino' to 'Tessin'\n",
      "Updated documents with canton 'Genève' to 'Genf'\n",
      "Updated documents with canton 'Fribourg/Freiburg' to 'Freiburg'\n"
     ]
    }
   ],
   "source": [
    "canton_translation = {\n",
    "    \"Vaud\": \"Waadt\",\n",
    "    \"Graubünden/Grischun/Grigioni\": \"Graubünden\",\n",
    "    \"Bern/Berne\": \"Bern\",\n",
    "    \"Valais/Wallis\": \"Wallis\",\n",
    "    \"Neuchâtel\": \"Neuenburg\",\n",
    "    \"Ticino\": \"Tessin\",\n",
    "    \"Genève\": \"Genf\",\n",
    "    \"Fribourg/Freiburg\": \"Freiburg\"\n",
    "}\n",
    "\n",
    "\n",
    "def translate_canton(canton_translation):\n",
    "    for key, value in canton_translation.items():\n",
    "        collection.update_many({\"node.canton\": key}, {\"$set\": {\"node.canton\": value}})\n",
    "        print(f\"Updated documents with canton '{key}' to '{value}'\")\n",
    "\n",
    "\n",
    "translate_canton(canton_translation)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-17T17:06:07.346092800Z"
    }
   },
   "id": "900763e3ce556531"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adding geojson location to the documents for spatial queries."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5198ec200bef390"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location field added to all documents.\n"
     ]
    }
   ],
   "source": [
    "def add_location_to_documents():\n",
    "    for document in collection.find():\n",
    "        lat = float(document['node']['lat'])\n",
    "        lon = float(document['node']['lon'])\n",
    "\n",
    "        # Erstellen des GeoJSON-Objekts\n",
    "        location = {\n",
    "            \"type\": \"Point\",\n",
    "            \"coordinates\": [lon, lat]  # Längen- und Breitengrad\n",
    "        }\n",
    "\n",
    "        # Aktualisieren des Dokuments in der Datenbank\n",
    "        collection.update_one(\n",
    "            {\"_id\": document['_id']},\n",
    "            {\"$set\": {\"node.location\": location}}\n",
    "        )\n",
    "\n",
    "    print(\"Location field added to all documents.\")\n",
    "    \n",
    "add_location_to_documents()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T10:58:53.416991Z",
     "start_time": "2023-11-18T10:58:53.322596400Z"
    }
   },
   "id": "a8d14073503e1cfe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Enrichment of the data with compressed air stations in the city of Zurich."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8e5d6bbce408fa2"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'node': {'amenity': 'compressed_air', 'id': '1011', 'lat': '47.3917066531', 'lon': '8.5185306277', 'canton': 'Zürich', 'location': {'type': 'Point', 'coordinates': [8.5185306277, 47.3917066531]}}}\n"
     ]
    }
   ],
   "source": [
    "data = json.load(open(\"../data/taz.velopumpstationen_p.json\", \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "def transform_feature(feature):\n",
    "    # Extrahiert und transformiert ein einzelnes Feature\n",
    "    transformed = {\n",
    "        \"amenity\": \"compressed_air\",\n",
    "        \"id\": str(feature[\"properties\"][\"id1\"]),\n",
    "        \"lat\": str(feature[\"geometry\"][\"coordinates\"][1]),\n",
    "        \"lon\": str(feature[\"geometry\"][\"coordinates\"][0]),\n",
    "        \"canton\": \"Zürich\",\n",
    "        \"location\": {\n",
    "            \"type\": \"Point\",\n",
    "            \"coordinates\": feature[\"geometry\"][\"coordinates\"]\n",
    "        }\n",
    "    }\n",
    "    return {\"node\": transformed}\n",
    "\n",
    "def transform_json(input_data):\n",
    "    # Transformiert die gesamte FeatureCollection\n",
    "    transformed_features = [transform_feature(feature) for feature in input_data[\"features\"]]\n",
    "    return transformed_features\n",
    "\n",
    "# Transformation durchführen\n",
    "transformed_json = transform_json(data)\n",
    "\n",
    "print(transformed_json[0])\n",
    "\n",
    "json.dump(transformed_json, open(\"../data/compressed_air_transformed.json\", \"w\", encoding=\"utf-8\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T10:26:04.495008500Z",
     "start_time": "2023-11-18T10:26:04.465728100Z"
    }
   },
   "id": "c73eaefd8edb48a8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Scrape data of Gemeinden > 10000 Einwohner from Wikipedia."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6439b2959282677"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import data_preprocessing_tools.webScraper as ws\n",
    "\n",
    "scraper = ws.SwissCommuneScraper()\n",
    "\n",
    "if scraper.fetch_data():\n",
    "    scraper.save_data('../../data/raw_data/grosse_gemeinden_data.json')\n",
    "    \n",
    "def load_data_in_db(data):\n",
    "    db = MongoClient(\"mongodb://localhost:27017/\")[\"data_base_OSM\"]\n",
    "    collection = db[\"bike_ways\"]\n",
    "    collection.insert_many(data)\n",
    "    print(\"Inserted documents into database\")\n",
    "    \n",
    "load_data_in_db(scraper.data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cd4f09183e96af"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fetch and load bicycle ways in collection bike_ways"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "779805309a3c9d4e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import data_preprocessing_tools.cycleWays as cw\n",
    "\n",
    "gemeinden = collection.distinct(\"Gemeinde\")\n",
    "\n",
    "for i in range(len(gemeinden)):\n",
    "    bicycle_ways = cw.BicycleWays(gemeinden[i])\n",
    "    print(f\"Gesamtlänge der Fahrradwege in {gemeinden[i]}: {bicycle_ways.total_cycleway_length} km\")\n",
    "    collection.update_one({\"Gemeinde\": gemeinden[i]},\n",
    "                            {\"$set\": {\"Fahrradwege in km\": bicycle_ways.total_cycleway_length}})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35cffbdb83caa42b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
